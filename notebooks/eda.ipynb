{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = r\"C:\\Users\\paulw\\Documents\\graduation_competition\\data\\train.csv\"\n",
    "test_path = r\"C:\\Users\\paulw\\Documents\\graduation_competition\\data\\test.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "Graduate    0.474163\n",
       "Dropout     0.330589\n",
       "Enrolled    0.195248\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[target].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Enrolled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76513</th>\n",
       "      <td>76513</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76514</th>\n",
       "      <td>76514</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76515</th>\n",
       "      <td>76515</td>\n",
       "      <td>Enrolled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76516</th>\n",
       "      <td>76516</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76517</th>\n",
       "      <td>76517</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76518 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    Target\n",
       "0          0  Graduate\n",
       "1          1   Dropout\n",
       "2          2   Dropout\n",
       "3          3  Enrolled\n",
       "4          4  Graduate\n",
       "...      ...       ...\n",
       "76513  76513  Graduate\n",
       "76514  76514  Graduate\n",
       "76515  76515  Enrolled\n",
       "76516  76516   Dropout\n",
       "76517  76517  Graduate\n",
       "\n",
       "[76518 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricals = [\n",
    "    \"Marital status\",\n",
    "    \"Application mode\",\n",
    "    \"Application order\",\n",
    "    \"Course\",\n",
    "    \"Daytime/evening attendance\",\n",
    "    \"Previous qualification\",\n",
    "    \"Nacionality\",\n",
    "    \"Mother's qualification\",\n",
    "    \"Father's qualification\",\n",
    "    \"Mother's occupation\",\n",
    "    \"Father's occupation\",\n",
    "    \"Displaced\",\n",
    "    \"Educational special needs\",\n",
    "    \"Debtor\",\n",
    "    \"Tuition fees up to date\",\n",
    "    \"Gender\",\n",
    "    \"Scholarship holder\",\n",
    "    \"International\"\n",
    "]\n",
    "\n",
    "numericals = [\n",
    "    \"Previous qualification (grade)\",\n",
    "    \"Admission grade\",\n",
    "    \"Age at enrollment\",\n",
    "    \"Curricular units 1st sem (credited)\",\n",
    "    \"Curricular units 1st sem (enrolled)\",\n",
    "    \"Curricular units 1st sem (evaluations)\",\n",
    "    \"Curricular units 1st sem (approved)\",\n",
    "    \"Curricular units 1st sem (grade)\",\n",
    "    \"Curricular units 1st sem (without evaluations)\",\n",
    "    \"Curricular units 2nd sem (credited)\",\n",
    "    \"Curricular units 2nd sem (enrolled)\",\n",
    "    \"Curricular units 2nd sem (evaluations)\",\n",
    "    \"Curricular units 2nd sem (approved)\",\n",
    "    \"Curricular units 2nd sem (grade)\",\n",
    "    \"Curricular units 2nd sem (without evaluations)\",\n",
    "    \"Unemployment rate\",\n",
    "    \"Inflation rate\",\n",
    "    \"GDP\"\n",
    "]\n",
    "features = categoricals + numericals\n",
    "train[[x for x in train.columns if x not in features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marital status                 6\n",
       "Application mode              22\n",
       "Application order              8\n",
       "Course                        19\n",
       "Daytime/evening attendance     2\n",
       "Previous qualification        21\n",
       "Nacionality                   18\n",
       "Mother's qualification        35\n",
       "Father's qualification        39\n",
       "Mother's occupation           40\n",
       "Father's occupation           56\n",
       "Displaced                      2\n",
       "Educational special needs      2\n",
       "Debtor                         2\n",
       "Tuition fees up to date        2\n",
       "Gender                         2\n",
       "Scholarship holder             2\n",
       "International                  2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[categoricals].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Previous qualification (grade)                     110\n",
       "Admission grade                                    668\n",
       "Age at enrollment                                   46\n",
       "Curricular units 1st sem (credited)                 21\n",
       "Curricular units 1st sem (enrolled)                 24\n",
       "Curricular units 1st sem (evaluations)              36\n",
       "Curricular units 1st sem (approved)                 23\n",
       "Curricular units 1st sem (grade)                  1206\n",
       "Curricular units 1st sem (without evaluations)      12\n",
       "Curricular units 2nd sem (credited)                 20\n",
       "Curricular units 2nd sem (enrolled)                 22\n",
       "Curricular units 2nd sem (evaluations)              31\n",
       "Curricular units 2nd sem (approved)                 21\n",
       "Curricular units 2nd sem (grade)                  1234\n",
       "Curricular units 2nd sem (without evaluations)      11\n",
       "Unemployment rate                                   11\n",
       "Inflation rate                                      13\n",
       "GDP                                                 11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[numericals].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "# set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "class MinShifter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.min: int = 0\n",
    "\n",
    "    def fit(self, X: np.ndarray):\n",
    "        self.min = X.min()\n",
    "\n",
    "    def transform(self, X: np.ndarray):\n",
    "        return X - self.min\n",
    "\n",
    "class DatasetModel(BaseModel):\n",
    "\n",
    "    numerical_feature_names: list[str]\n",
    "    categorical_feature_names: list[str]\n",
    "    numerical_features: np.ndarray\n",
    "    categorical_features: np.ndarray\n",
    "    target: Optional[np.ndarray] = None\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "class PreProcessingPipeline:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset: pd.DataFrame,\n",
    "            target_name: str,\n",
    "            categorical_features: list[str],\n",
    "            numerical_features: list[str],\n",
    "            numerical_scaler: StandardScaler = StandardScaler,\n",
    "            one_hot_encoder: OneHotEncoder = OneHotEncoder,\n",
    "            val_split: float = 0.2,\n",
    "            stratify: bool = True,\n",
    "    ):\n",
    "        self.dataset: pd.DataFrame = dataset.copy()\n",
    "        self.target_name: str = target_name\n",
    "        self.categorical_features: list[str] = categorical_features\n",
    "        self.numerical_features: list[str] = numerical_features\n",
    "        self.numerical_scaler: StandardScaler = numerical_scaler()\n",
    "        self.one_hot_encoder: OneHotEncoder = one_hot_encoder()\n",
    "        self.val_split: float = val_split\n",
    "        self.stratify: bool = stratify\n",
    "        self.categorical_transformers: dict[str, MinShifter] = {\n",
    "            feature: MinShifter() for feature in self.categorical_features\n",
    "        }\n",
    "\n",
    "    def split(self):\n",
    "        X = self.dataset.drop(columns=[self.target_name])\n",
    "        y = self.dataset[self.target_name]\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=self.val_split, stratify=y if self.stratify else None\n",
    "        )\n",
    "\n",
    "        return X_train, X_val, y_train, y_val\n",
    "    \n",
    "    def transform_target(self, y: np.ndarray, fit:bool = False)->np.ndarray:\n",
    "        if fit:\n",
    "            self.one_hot_encoder.fit(y.reshape(-1, 1))\n",
    "        return self.one_hot_encoder.transform(y.reshape(-1, 1))\n",
    "    \n",
    "    def scale_numericals(self, X: np.ndarray, fit:bool = False)->np.ndarray:\n",
    "        if fit:\n",
    "            self.numerical_scaler.fit(X)\n",
    "        return self.numerical_scaler.transform(X)\n",
    "    \n",
    "    def prepare_categoricals(self, X: np.ndarray, fit:bool = False)->np.ndarray:\n",
    "        for k, feature in enumerate(self.categorical_features):\n",
    "            if fit:\n",
    "                self.categorical_transformers[feature].fit(X[:, k])\n",
    "            X[:, k] = self.categorical_transformers[feature].transform(X[:, k])\n",
    "        return X\n",
    "    \n",
    "    def _run_pipeline(self, dataset: DatasetModel, fit: bool)->DatasetModel:\n",
    "        if dataset.target is not None:\n",
    "            dataset.target = self.transform_target(dataset.target, fit)\n",
    "        dataset.numerical_features = self.scale_numericals(dataset.numerical_features, fit)\n",
    "        dataset.categorical_features = self.prepare_categoricals(dataset.categorical_features, fit)\n",
    "        return dataset\n",
    "    \n",
    "    def dataset_pipeline(self)->tuple[DatasetModel, DatasetModel]:\n",
    "        X_train, X_val, y_train, y_val = self.split()\n",
    "        \n",
    "        self.train = DatasetModel(\n",
    "            numerical_feature_names=self.numerical_features,\n",
    "            categorical_feature_names=self.categorical_features,\n",
    "            numerical_features=X_train[self.numerical_features].values,\n",
    "            categorical_features=X_train[self.categorical_features].values,\n",
    "            target=y_train.values\n",
    "        )\n",
    "\n",
    "        self._run_pipeline(self.train, True)\n",
    "\n",
    "        self.val = DatasetModel(\n",
    "            numerical_feature_names=self.numerical_features,\n",
    "            categorical_feature_names=self.categorical_features,\n",
    "            numerical_features=X_val[self.numerical_features].values,\n",
    "            categorical_features=X_val[self.categorical_features].values,\n",
    "            target=y_val.values\n",
    "        )\n",
    "\n",
    "        self._run_pipeline(self.val, False)\n",
    "\n",
    "        return self.train, self.val\n",
    "    \n",
    "    def run_pipeline_new_data(self, data: pd.DataFrame)->DatasetModel:\n",
    "        dataset = DatasetModel(\n",
    "            numerical_feature_names=self.numerical_features,\n",
    "            categorical_feature_names=self.categorical_features,\n",
    "            numerical_features=data[self.numerical_features].values,\n",
    "            categorical_features=data[self.categorical_features].values,\n",
    "        )\n",
    "\n",
    "        return self._run_pipeline(dataset, False)\n",
    "    \n",
    "    def get_class_labels(self, predictions: np.ndarray)->np.ndarray:\n",
    "        output_classes = self.one_hot_encoder.categories_[0].shape[0]\n",
    "        predictions_ints = np.argmax(predictions, axis=1)\n",
    "        predictions_ohe = np.zeros((len(predictions_ints), output_classes))\n",
    "        predictions_ohe[np.arange(len(predictions_ints)), predictions_ints] = 1\n",
    "        labels = self.one_hot_encoder.inverse_transform(predictions_ohe)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = PreProcessingPipeline(\n",
    "    dataset=train,\n",
    "    target_name=target,\n",
    "    categorical_features=categoricals,\n",
    "    numerical_features=numericals,\n",
    "    val_split=0.2\n",
    ")\n",
    "\n",
    "train_data, val_data = preprocessing.dataset_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "# set seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class CategoryEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            dropout: float = 0.1,\n",
    "            init_mode: str = \"fan_out\",\n",
    "            init_nonlinearity: str = \"relu\"\n",
    "    ):\n",
    "        super(CategoryEmbedding, self).__init__()\n",
    "        self.embedding_dim = num_embeddings // 2\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.num_embeddings,\n",
    "            embedding_dim=self.embedding_dim\n",
    "        )\n",
    "        nn.init.kaiming_normal_(self.embedding.weight, mode=init_mode, nonlinearity=init_nonlinearity)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = torch.where(x < self.num_embeddings, x, torch.tensor(self.num_embeddings - 1, dtype=torch.long))\n",
    "        x = torch.where(x < 0, x, torch.tensor(0, dtype=torch.long))\n",
    "        # return self.dropout(self.relu(self.embedding(x)))\n",
    "        return self.dropout(self.embedding(x))\n",
    "\n",
    "    def trainable(self, trainable: bool):\n",
    "        self.embedding.weight.requires_grad = trainable\n",
    "\n",
    "class FFUnit(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            dropout: float = 0.1,\n",
    "            init_mode: str = \"fan_in\",\n",
    "            init_nonlinearity: str = \"relu\"\n",
    "    ):\n",
    "        super(FFUnit, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        nn.init.kaiming_normal_(self.linear.weight, mode=init_mode, nonlinearity=init_nonlinearity)\n",
    "        self.batch_norm = nn.BatchNorm1d(out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.dropout(self.activation(self.batch_norm(self.linear(x))))\n",
    "\n",
    "class ANN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            numerical_features: int,\n",
    "            categorical_max_values: list[int],\n",
    "            output_classes: int,\n",
    "            hidden_units: list[int],\n",
    "            dropout: float = 0.05\n",
    "    ):\n",
    "        super(ANN, self).__init__()\n",
    "        self.categorical_embeddings = nn.ModuleList([\n",
    "            CategoryEmbedding(max_val + 1, dropout) for max_val in categorical_max_values\n",
    "        ])\n",
    "        self.numerical_features = numerical_features\n",
    "        # self.numerical_ff_unit = FFUnit(\n",
    "        #     in_features=numerical_features,\n",
    "        #     out_features=numerical_features,\n",
    "        #     dropout=dropout,\n",
    "        #     init_mode=\"fan_out\",\n",
    "        # )\n",
    "        self.categorical_features = sum(embedding.embedding_dim for embedding in self.categorical_embeddings)\n",
    "        # self.categorical_ff_unit = FFUnit(\n",
    "        #     in_features=self.categorical_features,\n",
    "        #     out_features=self.categorical_features,\n",
    "        #     dropout=dropout\n",
    "        # )\n",
    "        self.hidden_units = hidden_units\n",
    "        self.ff_units = nn.ModuleList([\n",
    "            FFUnit(\n",
    "                in_features=self.numerical_features + self.categorical_features,\n",
    "                out_features=hidden_units[0],\n",
    "                dropout=dropout,\n",
    "                init_mode=\"fan_in\",\n",
    "            )\n",
    "        ])\n",
    "        for i in range(1, len(hidden_units)):\n",
    "            self.ff_units.append(\n",
    "                FFUnit(\n",
    "                    in_features=hidden_units[i - 1],\n",
    "                    out_features=hidden_units[i],\n",
    "                    dropout=dropout,\n",
    "                    init_mode=\"fan_in\",\n",
    "                )\n",
    "            )\n",
    "        self.output = nn.Linear(hidden_units[-1], output_classes)\n",
    "        nn.init.kaiming_normal_(self.output.weight, mode=\"fan_in\")\n",
    "        self.output_activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, numerical: torch.Tensor, categorical: torch.Tensor):\n",
    "        assert categorical.shape[1] == len(self.categorical_embeddings), \"Mismatch between input data and model embeddings: {} != {}\".format(categorical.shape[1], len(self.categorical_embeddings))\n",
    "        embeddings = [self.categorical_embeddings[i](categorical[:, i]) for i in range(len(self.categorical_embeddings))]\n",
    "        embeddings = torch.cat(embeddings, dim=1)\n",
    "        # embeddings = self.categorical_ff_unit(embeddings)\n",
    "        # numerical = self.numerical_ff_unit(numerical)\n",
    "        x = torch.cat([numerical, embeddings], dim=1)\n",
    "        for unit in self.ff_units:\n",
    "            x = unit(x)\n",
    "        logits = self.output(x)\n",
    "        return logits\n",
    "\n",
    "    def predict(self, numerical: torch.Tensor, categorical: torch.Tensor):\n",
    "        return self.output_activation(self(numerical, categorical))\n",
    "    \n",
    "    def trainable_embeddings(self, trainable: bool):\n",
    "        for embedding in self.categorical_embeddings:\n",
    "            embedding.trainable(trainable)\n",
    "\n",
    "    def _create_config(self) -> dict:\n",
    "        categorical_max_values = [int(emb.num_embeddings - 1) for emb in self.categorical_embeddings]\n",
    "        return {\n",
    "            \"numerical_features\": self.numerical_features,\n",
    "            \"categorical_max_values\": categorical_max_values,\n",
    "            \"hidden_units\": self.hidden_units,\n",
    "            \"output_classes\": self.output.out_features\n",
    "        }\n",
    "\n",
    "    def save_pretrained(self, model_save_path: str):\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "        # Save model state\n",
    "        torch.save(self.state_dict(), f\"{model_save_path}/model.pth\")\n",
    "\n",
    "        # Save config\n",
    "        config_path = f\"{model_save_path}/config.json\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self._create_config(), f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_pretrained(cls, model_save_path: str):\n",
    "        # Load config\n",
    "        config_path = f\"{model_save_path}/config.json\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # Create model\n",
    "        model = cls(\n",
    "            numerical_features=config[\"numerical_features\"],\n",
    "            categorical_max_values=config[\"categorical_max_values\"],\n",
    "            hidden_units=config[\"hidden_units\"],\n",
    "            output_classes=config[\"output_classes\"]\n",
    "        )\n",
    "\n",
    "        # Load model state\n",
    "        model_path = f\"{model_save_path}/model.pth\"\n",
    "        map_location = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location=map_location))\n",
    "\n",
    "        return model\n",
    "    \n",
    "\n",
    "class Dataset(Dataset):\n",
    "    \n",
    "        def __init__(\n",
    "                self,\n",
    "                numerical_features: np.ndarray,\n",
    "                categorical_features: np.ndarray,\n",
    "                target: Optional[np.ndarray] = None\n",
    "        ):\n",
    "            self.numerical_features = numerical_features\n",
    "            self.categorical_features = categorical_features\n",
    "            self.target = target\n",
    "    \n",
    "        def __len__(self):\n",
    "            return self.numerical_features.shape[0]\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            if self.target is not None:\n",
    "                return {\n",
    "                    \"numerical\": torch.tensor(self.numerical_features[idx], dtype=torch.float32),\n",
    "                    \"categorical\": torch.tensor(self.categorical_features[idx], dtype=torch.long),\n",
    "                    \"target\": torch.tensor(self.target.toarray()[idx], dtype=torch.float32)\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"numerical\": torch.tensor(self.numerical_features[idx], dtype=torch.float32),\n",
    "                    \"categorical\": torch.tensor(self.categorical_features[idx], dtype=torch.long)\n",
    "                }\n",
    "        \n",
    "train_dataset = Dataset(\n",
    "    numerical_features=train_data.numerical_features,\n",
    "    categorical_features=train_data.categorical_features,\n",
    "    target=train_data.target\n",
    ")\n",
    "\n",
    "val_dataset = Dataset(\n",
    "    numerical_features=val_data.numerical_features,\n",
    "    categorical_features=val_data.categorical_features,\n",
    "    target=val_data.target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulw\\Documents\\graduation_competition\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            optimizer: torch.optim.Optimizer,\n",
    "            criterion: nn.Module,\n",
    "            train_dataset: Dataset,\n",
    "            val_dataset: Dataset,\n",
    "            model_save_path: str,\n",
    "            batch_size: int = 32,\n",
    "            n_epochs: int = 100,\n",
    "            gradient_accumulation_steps: int = 1,\n",
    "            eval_steps: int|float = 10,\n",
    "            early_stopping_patience: int = 3\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.train_loader: DataLoader = None\n",
    "        self.val_loader: DataLoader = None\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.eval_steps = eval_steps\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.best_loss: float = float(\"inf\")\n",
    "        self.model_save_path = model_save_path\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if not os.path.exists(self.model_save_path):\n",
    "            os.makedirs(self.model_save_path)\n",
    "\n",
    "    def _process_batch(\n",
    "            self,\n",
    "            batch: dict, \n",
    "            step: int\n",
    "    ):\n",
    "        numerical = batch[\"numerical\"]\n",
    "        categorical = batch[\"categorical\"]\n",
    "        target = batch[\"target\"]\n",
    "        \n",
    "        numerical = numerical.to(self.device)\n",
    "        categorical = categorical.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(numerical, categorical)\n",
    "        loss = self.criterion(output, target)\n",
    "        loss.backward()\n",
    "        if step % self.gradient_accumulation_steps == 0:\n",
    "            self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def _evaluate(\n",
    "            self\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(enumerate(self.val_loader), total=len(self.val_loader), desc=f\"Validating...\", leave=False, position=1)\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                progress_bar.update(1)\n",
    "                numerical = batch[\"numerical\"]\n",
    "                categorical = batch[\"categorical\"]\n",
    "                target = batch[\"target\"]\n",
    "                numerical = numerical.to(self.device)\n",
    "                categorical = categorical.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "                output = self.model(numerical, categorical)\n",
    "                output = output.cpu()\n",
    "                target = target.cpu()\n",
    "                loss = self.criterion(output, target)\n",
    "                targets.extend(target.numpy())\n",
    "                predictions.extend(output.numpy())\n",
    "                total_loss += loss.item()\n",
    "        return targets, predictions, total_loss / len(self.val_loader)\n",
    "    \n",
    "    def _run_epoch(\n",
    "            self,\n",
    "            epoch: int,\n",
    "            step: int,\n",
    "            patience: int\n",
    "    ):\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(enumerate(self.train_loader), total=len(self.train_loader), desc=f\"Epoch {epoch+1}\", leave=False, position=0)\n",
    "        for batch in self.train_loader:\n",
    "            progress_bar.update(1)\n",
    "            loss = self._process_batch(batch, step)\n",
    "            total_loss += loss\n",
    "            if step % self.eval_steps == 0 and step != 0:\n",
    "                targets, predictions, val_loss = self._evaluate()\n",
    "                # convert prediction logits to class\n",
    "                predictions = np.argmax(predictions, axis=1)\n",
    "                targets = np.argmax(targets, axis=1)\n",
    "                accuracy = accuracy_score(targets, predictions)\n",
    "                f1 = f1_score(targets, predictions, average=\"macro\")\n",
    "                print(\"=\"*50)\n",
    "                print(\"=\"*50)\n",
    "                print(f\"Epoch: {epoch+1}\")\n",
    "                print(f\"Training Loss: {total_loss / self.eval_steps}\")\n",
    "                print(f\"Validation Loss: {val_loss}, Accuracy: {accuracy}, F1: {f1}\")\n",
    "                print(\"=\"*50)\n",
    "                print(\"=\"*50)\n",
    "                if val_loss < self.best_loss:\n",
    "                    print(\"** NEW BEST MODEL FOUND! **\")\n",
    "                    self.best_loss = val_loss\n",
    "                    patience = 0\n",
    "                    self.model.save_pretrained(self.model_save_path)\n",
    "                else:\n",
    "                    patience += 1\n",
    "                if patience == self.early_stopping_patience:\n",
    "                    break\n",
    "                total_loss = 0\n",
    "            step += 1\n",
    "        return step, patience\n",
    "    \n",
    "    def train(\n",
    "            self\n",
    "    ):\n",
    "        step = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "        patience = 0\n",
    "        early_stopped = False\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        if isinstance(self.eval_steps, float):\n",
    "            self.eval_steps = int((len(train_dataset) / self.batch_size) * self.eval_steps)\n",
    "        for epoch in range(self.n_epochs):\n",
    "            step, patience = self._run_epoch(epoch, step, patience)\n",
    "            if epoch == 1:\n",
    "                self.model.trainable_embeddings(False)\n",
    "            if patience == self.early_stopping_patience:\n",
    "                early_stopped = True\n",
    "                break\n",
    "        if early_stopped:\n",
    "            print(\"** EARLY STOPPING... **\")\n",
    "            self.model = ANN.load_pretrained(self.model_save_path)\n",
    "        else:\n",
    "            self.model.save_pretrained(self.model_save_path)\n",
    "        return self.model\n",
    "\n",
    "numerical_features = train_data.numerical_features.shape[1]\n",
    "categorical_features = train_data.categorical_features.shape[1]\n",
    "output_classes = train_data.target.shape[1]\n",
    "hidden_units = [int(128), int(64), int(32)]\n",
    "\n",
    "model = ANN(\n",
    "    numerical_features=numerical_features,\n",
    "    categorical_max_values=[\n",
    "        train_data.categorical_features[:, i].max() \n",
    "        for i in range(categorical_features)\n",
    "    ],\n",
    "    output_classes=output_classes,\n",
    "    hidden_units=hidden_units\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    model_save_path=\"./model\",\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_steps=0.2,\n",
    "    early_stopping_patience=5,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  20%|â–ˆâ–ˆ        | 383/1913 [03:19<13:23,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 1\n",
      "Training Loss: 0.8275176863239698\n",
      "Validation Loss: 0.6670208794214532, Accuracy: 0.770321484579195, F1: 0.7406429906611834\n",
      "==================================================\n",
      "==================================================\n",
      "** NEW BEST MODEL FOUND! **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 765/1913 [06:43<10:04,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 1\n",
      "Training Loss: 0.6371946043837133\n",
      "Validation Loss: 0.6035874925227155, Accuracy: 0.7763983272347099, F1: 0.7484631799212819\n",
      "==================================================\n",
      "==================================================\n",
      "** NEW BEST MODEL FOUND! **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1147/1913 [10:06<06:40,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 1\n",
      "Training Loss: 0.5987304458053324\n",
      "Validation Loss: 0.5971181910321708, Accuracy: 0.7658782017773131, F1: 0.7444814010582189\n",
      "==================================================\n",
      "==================================================\n",
      "** NEW BEST MODEL FOUND! **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1530/1913 [13:33<07:52,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 1\n",
      "Training Loss: 0.5878485025769753\n",
      "Validation Loss: 0.5981724078072884, Accuracy: 0.7802535284892839, F1: 0.7520908103429672\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1911/1913 [16:57<00:01,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 1\n",
      "Training Loss: 0.6012261218893591\n",
      "Validation Loss: 0.5960061316201483, Accuracy: 0.7960010454783063, F1: 0.7595544746369384\n",
      "==================================================\n",
      "==================================================\n",
      "** NEW BEST MODEL FOUND! **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  20%|â–ˆâ–‰        | 381/1913 [03:26<31:33,  1.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 2\n",
      "Training Loss: 0.6567351346084584\n",
      "Validation Loss: 0.6062277268641677, Accuracy: 0.77946941975954, F1: 0.7536386568969133\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 763/1913 [06:54<23:45,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 2\n",
      "Training Loss: 0.5950847332078125\n",
      "Validation Loss: 0.6063687797628017, Accuracy: 0.7417668583376895, F1: 0.7292720509409539\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1144/1913 [10:19<06:49,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 2\n",
      "Training Loss: 0.5770867892116777\n",
      "Validation Loss: 0.5826391320138983, Accuracy: 0.7735232618923157, F1: 0.7516618844513158\n",
      "==================================================\n",
      "==================================================\n",
      "** NEW BEST MODEL FOUND! **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1526/1913 [13:46<03:25,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 2\n",
      "Training Loss: 0.581522385955481\n",
      "Validation Loss: 0.5791259238142559, Accuracy: 0.7805802404600104, F1: 0.7566842375489321\n",
      "==================================================\n",
      "==================================================\n",
      "** NEW BEST MODEL FOUND! **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1909/1913 [17:17<00:04,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 2\n",
      "Training Loss: 0.5751915307257188\n",
      "Validation Loss: 0.5835602588344965, Accuracy: 0.7863303711447988, F1: 0.7582500350108181\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  20%|â–ˆâ–‰        | 382/1913 [00:14<03:52,  6.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 3\n",
      "Training Loss: 0.6220131209851559\n",
      "Validation Loss: 0.5922149558530222, Accuracy: 0.78254051228437, F1: 0.7570481626743102\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 766/1913 [00:27<02:54,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 3\n",
      "Training Loss: 0.5854725807671147\n",
      "Validation Loss: 0.5840131774824696, Accuracy: 0.7965237846314689, F1: 0.7626204820568044\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1138/1913 [00:38<00:22, 34.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 3\n",
      "Training Loss: 0.5873049345862179\n",
      "Validation Loss: 0.5769317035013052, Accuracy: 0.7846968112911658, F1: 0.7594784795425534\n",
      "==================================================\n",
      "==================================================\n",
      "** NEW BEST MODEL FOUND! **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1521/1913 [00:52<00:11, 34.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 3\n",
      "Training Loss: 0.5656037951639186\n",
      "Validation Loss: 0.5752002145153994, Accuracy: 0.7899242028227914, F1: 0.7621432082856682\n",
      "==================================================\n",
      "==================================================\n",
      "** NEW BEST MODEL FOUND! **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1904/1913 [01:06<00:00, 34.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 3\n",
      "Training Loss: 0.5752021444093495\n",
      "Validation Loss: 0.5751263611874152, Accuracy: 0.7837820177731312, F1: 0.7588371153603898\n",
      "==================================================\n",
      "==================================================\n",
      "** NEW BEST MODEL FOUND! **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  20%|â–ˆâ–‰        | 379/1913 [00:13<04:02,  6.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 4\n",
      "Training Loss: 0.5976190166791696\n",
      "Validation Loss: 0.5889303746452411, Accuracy: 0.7856116048092002, F1: 0.759475539688666\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 760/1913 [00:28<04:10,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 4\n",
      "Training Loss: 0.5802397942511823\n",
      "Validation Loss: 0.5829754978853875, Accuracy: 0.7976346053319394, F1: 0.7645719811903314\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1143/1913 [00:42<01:57,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 4\n",
      "Training Loss: 0.5812896348497006\n",
      "Validation Loss: 0.5839403085021734, Accuracy: 0.7971772085729221, F1: 0.763825851490694\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1527/1913 [00:55<00:57,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 4\n",
      "Training Loss: 0.5700094401836395\n",
      "Validation Loss: 0.5767281928366061, Accuracy: 0.78659174072138, F1: 0.7589958314118693\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1899/1913 [01:06<00:00, 34.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 4\n",
      "Training Loss: 0.5815176256196037\n",
      "Validation Loss: 0.5742836272293441, Accuracy: 0.792472556194459, F1: 0.764381010435668\n",
      "==================================================\n",
      "==================================================\n",
      "** NEW BEST MODEL FOUND! **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:  20%|â–ˆâ–‰        | 374/1913 [00:13<05:15,  4.87it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 5\n",
      "Training Loss: 0.5887955896829435\n",
      "Validation Loss: 0.5885224261463061, Accuracy: 0.7807109252483011, F1: 0.7567257115592372\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 757/1913 [00:27<04:25,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 5\n",
      "Training Loss: 0.5817614945519657\n",
      "Validation Loss: 0.5833222254457454, Accuracy: 0.7762676424464192, F1: 0.7521235049422194\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1141/1913 [00:41<02:21,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 5\n",
      "Training Loss: 0.5799596693503295\n",
      "Validation Loss: 0.5805773297655309, Accuracy: 0.7702561421850497, F1: 0.750554198732612\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1525/1913 [00:55<00:58,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 5\n",
      "Training Loss: 0.574587869511537\n",
      "Validation Loss: 0.579198559788921, Accuracy: 0.7729351803450079, F1: 0.7528466694807268\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Epoch: 5\n",
      "Training Loss: 0.5708548680181903\n",
      "Validation Loss: 0.5788962571630896, Accuracy: 0.7837820177731312, F1: 0.757680464882115\n",
      "==================================================\n",
      "==================================================\n",
      "** EARLY STOPPING... **\n"
     ]
    }
   ],
   "source": [
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Validation Loss: 0.5742836272293441\n",
      "Accuracy: 0.792472556194459\n",
      "F1: 0.764381010435668\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# model = ANN.load_pretrained(r\"C:\\Users\\paulw\\Documents\\graduation_competition\\model\")\n",
    "\n",
    "# use model on val_dataset\n",
    "model.eval()\n",
    "model.to(torch.device(\"cpu\"))\n",
    "targets = []\n",
    "predictions = []\n",
    "total_loss = 0\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=f\"Validating...\", leave=False, position=1)\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        progress_bar.update(1)\n",
    "        numerical = batch[\"numerical\"]\n",
    "        categorical = batch[\"categorical\"]\n",
    "        target = batch[\"target\"]\n",
    "        output = model(numerical, categorical)\n",
    "        output = output.cpu()\n",
    "        target = target.cpu()\n",
    "        loss = criterion(output, target)\n",
    "        targets.extend(target.numpy())\n",
    "        predictions.extend(output.numpy())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "# convert prediction logits to class\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "targets = np.argmax(targets, axis=1)\n",
    "accuracy = accuracy_score(targets, predictions)\n",
    "f1 = f1_score(targets, predictions, average=\"macro\")\n",
    "print(\"=\"*50)\n",
    "print(\"=\"*50)\n",
    "print(f\"Validation Loss: {total_loss / len(val_loader)}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(\"=\"*50)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(test_path)\n",
    "\n",
    "test_data = preprocessing.run_pipeline_new_data(test)\n",
    "\n",
    "test_dataset = Dataset(\n",
    "    numerical_features=test_data.numerical_features,\n",
    "    categorical_features=test_data.categorical_features,\n",
    "    target=test_data.target\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "predictions = []\n",
    "model.eval()\n",
    "model.to(torch.device(\"cpu\"))\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        numerical = batch[\"numerical\"]\n",
    "        categorical = batch[\"categorical\"]\n",
    "        output = model.predict(numerical, categorical)\n",
    "        predictions.extend(output.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = preprocessing.get_class_labels(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Target\"] = labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "test[[\"id\", \"Target\"]].to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
